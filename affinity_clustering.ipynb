{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c75679",
   "metadata": {},
   "source": [
    "## Tweakable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09509b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = dict(\n",
    "    \n",
    "    MAX_DOCS=5000,                    # for quick code testing - int or None (all docs)\n",
    "    MIN_LEN=3,                        # words less than this length will be filtered\n",
    "    MAX_LEN=100,                      # words more than this length will be filtered\n",
    "    \n",
    "    DISALLOWED_NERS=[                 # Named-entities to filter out\n",
    "                                      # See https://github.com/explosion/spaCy/blob/b7ba7f78a28ef71fca60415d0165e27a058d1946/spacy/glossary.py#L318\n",
    "        'PERSON',\n",
    "        'GPE',\n",
    "        'ORG'\n",
    "    ],\n",
    "    BIGRAM=False,                     # Form bigrams before creating corpus?\n",
    "    BIGRAM_MIN_PMI=5,                 # Min. PMI in order to create bigrams (determine by manual inspection of generated bigrams.txt)\n",
    "    BIGRAM_MIN_FREQ=20,               # Min. freq of co-occurring tokens before they can be considered a bigram\n",
    "\n",
    "    COMMON_WORDS_MAX_FREQUENCY=10000, # For root words, the max. frequency beyond which they're not useful\n",
    "    COMMON_WORDS_MAX_DOCS=0.5,        # For root words, max docs (absolute or relative) beyond which they're not useful\n",
    "    COMMON_WORDS_MIN_DOCS=5,          # For root words, min docs (absolute or relative) beyond which they're not useful\n",
    "\n",
    "    KEEP_TOKENS=[],                   # Root words to preserve in the vocabulary regardless of their frequency (high or low)\n",
    "\n",
    "\n",
    "    WORD2VEC_VECTOR_SIZE=200,\n",
    "    WORD2VEC_WINDOW=10,\n",
    "    WORD2VEC_EPOCHS=30,\n",
    "\n",
    "    AFFINITY_N_DOCS=None,\n",
    "    AFFINITY_DAMPING=0.7,\n",
    "    AFFINITY_MAX_ITER=400,\n",
    "\n",
    "    # Parameters that determine initial preference values\n",
    "\n",
    "    # How many tags to consider (most common to least common)\n",
    "    AFFINITY_PREFERENCE_N_TAGS=100,\n",
    "\n",
    "    # How many clusters to form based solely on the most common descriptions (before invoking AffinityModel)\n",
    "    N_PRECLUSTERS=30,\n",
    "\n",
    "    # How many documents with common descriptions to use as 'seed' documents for AffinityModel clustering?\n",
    "    AFFINITY_PREFERENCE_N_COMMON_DESCRIPTIONS=20,\n",
    "\n",
    "    # Default preference value for non-exemplar docs - None to auto-determine\n",
    "    AFFINITY_PREFERENCE_DEFAULT=-20,\n",
    "\n",
    "    # Default preference for exemplar docs - None to auto-determine\n",
    "    AFFINITY_PREFERENCE_EXEMPLAR=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b95726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vineetb/.conda/envs/geniza/lib/python3.8/site-packages/seaborn/rcmod.py:400: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "/home/vineetb/.conda/envs/geniza/lib/python3.8/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "import textwrap\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, Word2Vec\n",
    "import spacy\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from ldamallet import LdaMallet\n",
    "from utils import generate_cluster_size_figure\n",
    "\n",
    "\n",
    "logging.basicConfig(level=os.environ.get('LOGLEVEL', 'INFO'))\n",
    "logger = logging.getLogger(__name__)\n",
    "sns.set()\n",
    "\n",
    "stop_words = _stop_words + nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882ea89",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57b11343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words, disallowed_ners=None, min_len=3, max_len=30):\n",
    "    # python -m spacy download en_core_web_sm\n",
    "    # English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
    "    # Other pipelines at https://spacy.io/models/en\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    texts_out = []\n",
    "\n",
    "    # implement lemmatization and filter out unwanted part of speech tags\n",
    "    for i, sentence in enumerate(texts):\n",
    "        doc = nlp(sentence)\n",
    "        doctext = doc.text\n",
    "        ents = list(doc.ents)\n",
    "\n",
    "        if disallowed_ners is not None:\n",
    "            # Filtering out disallowed NERs should be done prior to splitting the sentence using whitespace.\n",
    "            disallowed_tokens = []\n",
    "            for ent in ents:\n",
    "                if ent.label_ in disallowed_ners:\n",
    "                    disallowed_tokens.append(ent.text.lower())\n",
    "\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "        # simple_preprocess => lowercase; ignore tokens that are too short or too long\n",
    "        tokens = [t for t in simple_preprocess(' '.join(tokens), deacc=False, min_len=min_len, max_len=max_len)\n",
    "                  if t not in stop_words and t not in disallowed_tokens]\n",
    "        texts_out.append(tokens)\n",
    "\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def plot_word2vec_model(model):\n",
    "    words = list(model.wv.key_to_index)\n",
    "    X = model.wv[words]\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(X)\n",
    "    plt.scatter(result[:, 0], result[:, 1])\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "\n",
    "def vectorize(list_of_docs, model):\n",
    "    features = np.zeros((len(list_of_docs), model.vector_size))\n",
    "    for i, tokens in enumerate(list_of_docs):\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if vectors:\n",
    "            features[i, :] = np.mean(vectors, axis=0)\n",
    "\n",
    "    return features\n",
    "\n",
    "def file_to_set(filepath, split_lines=True):\n",
    "    s = set() \n",
    "    lines = open(filepath, 'r').read().splitlines()\n",
    "    for line in lines:\n",
    "        line = line.strip().lower()\n",
    "        if line and not line.startswith('#'):\n",
    "            if split_lines:\n",
    "                for t in line.split():\n",
    "                    s.add(t)\n",
    "            else:\n",
    "                s.add(line) \n",
    "    return s\n",
    "\n",
    "\n",
    "def generate_html(docs_df, wv, af, X, output_dir, filename):\n",
    "    cluster_centers_indices = af.cluster_centers_indices_\n",
    "    n_clusters_ = len(cluster_centers_indices)\n",
    "    logger.info(f'Estimated number of clusters: {n_clusters_}')\n",
    "\n",
    "    n_preclusters = len(docs_df[docs_df.cluster_id != -1].cluster_id.unique())\n",
    "    # The dataframe representing the docs that af/X see\n",
    "    model_docs_df = docs_df[docs_df.cluster_id == -1]\n",
    "\n",
    "    # Shift label ids identified by the AffinityModel up\n",
    "    labels = n_preclusters + af.labels_\n",
    "\n",
    "    with open(os.path.join(output_dir, filename), 'w') as f:\n",
    "        for i in range(n_preclusters):\n",
    "            _df = docs_df[docs_df.cluster_id == i]\n",
    "            f.write(f'<hr/><b>Cluster {i} ({len(_df)} docs)</b><hr/>')\n",
    "            f.write(f'<i>Pre-cluster based on description:</i> {_df.iloc[0].description}<hr/>')\n",
    "            for _, row in _df.iterrows():\n",
    "                f.write(f'<a target=\"_blank\" href=\"{row.url}\">{row.pgpid}</a><br/>')\n",
    "\n",
    "        for label in np.unique(labels):\n",
    "            docs = np.where(labels == label)[0]\n",
    "            docs_mean_vector = X[docs].mean(axis=0)\n",
    "            terms = ', '.join([term for (term, _) in wv.most_similar(docs_mean_vector)])\n",
    "            f.write(f'<hr/><b>Cluster {label} ({len(docs)} docs)</b><hr/>')\n",
    "            f.write(f'<i>{terms}</i><hr/>')\n",
    "            for doc in docs:\n",
    "                row = model_docs_df.iloc[doc]  # important to use iloc here, not loc\n",
    "                f.write(f'<a target=\"_blank\" href=\"{row.url}\">{row.pgpid}</a><br/>')\n",
    "                f.write('Tags: <i>' + str(row.tags) + '</i>')\n",
    "                if row['is_exemplar']:\n",
    "                    f.write('<p style=\"color:red;\">' + str(row.description) + '</p>')\n",
    "                else:\n",
    "                    f.write('<p>' + str(row.description) + '</p>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaeb7a1",
   "metadata": {},
   "source": [
    "## Unique Run ID\n",
    "\n",
    "The following logic creates a unique Run ID. Parameter values, input and output files are copied to the `results/<run_ID>` folder. You can manually override the Run ID here if you wish to overwrite results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a949e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using Run ID 0008. Results will be stored in the results/0008 folder.\n"
     ]
    }
   ],
   "source": [
    "existing_runs = sorted([d for d in os.listdir('results') if os.path.isdir(f'results/{d}')])\n",
    "if existing_runs:\n",
    "    run_id = int(existing_runs[-1]) + 1\n",
    "else:\n",
    "    run_id = 1\n",
    "# run_id = 42  // Uncomment and specify an explicit Run ID here\n",
    "run_id = f'{run_id:04}'\n",
    "logger.info(f'Using Run ID {run_id}. Results will be stored in the results/{run_id} folder.')\n",
    "\n",
    "output_dir = os.path.join('results', run_id)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger.addHandler(logging.FileHandler(os.path.join(output_dir, 'log.txt')))\n",
    "with open(os.path.join(output_dir, 'params.txt'), 'w') as f:\n",
    "    pprint(PARAMS, f)\n",
    "\n",
    "_stop_words = []\n",
    "os.makedirs(f'{output_dir}/stopwords', exist_ok=True)\n",
    "for filename in glob('stopwords/*.txt'):\n",
    "    _stop_words.extend(list(file_to_set(filename)))\n",
    "    shutil.copy(filename, os.path.join(output_dir, filename))\n",
    "\n",
    "_bad_tags = []\n",
    "os.makedirs(f'{output_dir}/stoptags', exist_ok=True)\n",
    "for filename in glob('stoptags/*.txt'):\n",
    "    _bad_tags.extend(list(file_to_set(filename)))\n",
    "    shutil.copy(filename, os.path.join(output_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113aaaa",
   "metadata": {},
   "source": [
    "### Read Descriptions\n",
    "\n",
    "Add any calculated columns to the Dataframe here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdbbc45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:No. of records = 5000\n",
      "INFO:__main__:After dropping records with missing description, no. of records = 4967\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('descriptions.csv', dtype={'tags': str})[:PARAMS['MAX_DOCS']]\n",
    "logger.info(f'No. of records = {len(df)}')\n",
    "df = df.dropna(subset=['description'])\n",
    "logger.info(f'After dropping records with missing description, no. of records = {len(df)}')\n",
    "df['tags'] = df['tags'].str.lower()\n",
    "df['tags'].fillna('', inplace=True)\n",
    "\n",
    "# -------------- Add additional columns to Dataframe --------------- #\n",
    "df['cluster_id'] = -1  # will be populated by this script\n",
    "df['preference'] = 0\n",
    "df['is_exemplar'] = False\n",
    "# -------------- Add additional columns to Dataframe --------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6caf2cb",
   "metadata": {},
   "source": [
    "### Find common descriptions\n",
    "\n",
    "Very common descriptions should form their own individual clusters (\"pre-clusters\"). These mostly have descriptions of the form `image missing` / `see Gotein index` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = Counter()\n",
    "for i, row in df.iterrows():\n",
    "    descriptions.update([row.description])\n",
    "\n",
    "common_descriptions_with_counts = descriptions.most_common(PARAMS['N_PRECLUSTERS'] + PARAMS['AFFINITY_PREFERENCE_N_COMMON_DESCRIPTIONS'])\n",
    "common_descriptions = [t[0] for t in common_descriptions_with_counts]\n",
    "for i, desc in enumerate(common_descriptions[:PARAMS['N_PRECLUSTERS']]):\n",
    "    df.loc[df[df.description == desc].index, 'cluster_id'] = i\n",
    "\n",
    "df_complete = df.copy()\n",
    "# Filter these out from further processing\n",
    "df = df[df.cluster_id == -1]\n",
    "logger.info(f'After filtering out {PARAMS[\"N_PRECLUSTERS\"]} most common descriptions, no. of records = {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e9bd94",
   "metadata": {},
   "source": [
    "### Find common tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10986ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = Counter()\n",
    "tag_dict = {}\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        _tags = row.tags.split(',')\n",
    "    except:\n",
    "        continue\n",
    "    else:\n",
    "        for tag in _tags:\n",
    "            tag = tag.strip().lower().replace(':', '').replace('.', '').replace(';', '').replace('(', '').replace(')', '')\n",
    "            if tag not in _bad_tags:\n",
    "                tags.update([tag])\n",
    "\n",
    "common_tags = [t[0] for t in tags.most_common(PARAMS['AFFINITY_PREFERENCE_N_TAGS'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ee709",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pkl_file = os.path.join(output_dir, 'data.pik')\n",
    "if not os.path.exists(data_pkl_file):\n",
    "    data = list(df.description)\n",
    "    data = process_words(\n",
    "        data,\n",
    "        stop_words=stop_words,\n",
    "        disallowed_ners=PARAMS['DISALLOWED_NERS'],\n",
    "        min_len=PARAMS['MIN_LEN'],\n",
    "        max_len=PARAMS['MAX_LEN']\n",
    "    )\n",
    "    logger.info(f'After filtering stopwords/short words/lemmatization, no. of records = {len(data)}')\n",
    "\n",
    "    with open(data_pkl_file, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(data_pkl_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "if not os.path.exists(os.path.join(output_dir, 'wvmodel.bin')):\n",
    "    model = Word2Vec(\n",
    "        sentences=data,\n",
    "        vector_size=PARAMS['WORD2VEC_VECTOR_SIZE'],\n",
    "        workers=8,\n",
    "        sg=1,\n",
    "        window=PARAMS['WORD2VEC_WINDOW'],\n",
    "        epochs=PARAMS['WORD2VEC_EPOCHS']\n",
    "    )\n",
    "    model.save(os.path.join(output_dir, 'wvmodel.bin'))\n",
    "    with open(os.path.join(output_dir, 'wvmodel_keys.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(model.wv.index_to_key))\n",
    "else:\n",
    "    model = Word2Vec.load(os.path.join(output_dir, 'wvmodel.bin'))\n",
    "\n",
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = PARAMS['AFFINITY_N_DOCS']\n",
    "X = vectorize(data[:n_docs], model=model)\n",
    "logger.info(f'Vectorization of {n_docs} documents done.')\n",
    "\n",
    "_default_preference = PARAMS['AFFINITY_PREFERENCE_DEFAULT']\n",
    "_exemplar_preference = PARAMS['AFFINITY_PREFERENCE_EXEMPLAR']\n",
    "\n",
    "if _default_preference is None and _exemplar_preference is None:\n",
    "    logger.info('Fitting AffinityPropagation model to documents..')\n",
    "    af = AffinityPropagation(verbose=True, damping=PARAMS['AFFINITY_DAMPING']).fit(X)\n",
    "    generate_html(df_complete, wv, af, X, output_dir, 'affinity_clustering.html')\n",
    "    generate_cluster_size_figure(af=af, output_dir=output_dir, filename='affinity_clustering.png')\n",
    "\n",
    "    # If we didn't specify starting preference values, it would have been the median (for all data points):\n",
    "    median_preference = np.median(af.affinity_matrix_)\n",
    "    logger.info(f'Median Preference Value of AF model: {median_preference}')\n",
    "    min_preference = np.min(af.affinity_matrix_)\n",
    "    logger.info(f'Min Preference Value of AF model: {min_preference}')\n",
    "    max_preference = np.max(af.affinity_matrix_)\n",
    "    logger.info(f'Max Preference Value of AF model: {max_preference}')\n",
    "\n",
    "    exemplar_preference = max_preference\n",
    "    df['preference'] = min_preference - (max_preference - min_preference)\n",
    "else:\n",
    "    exemplar_preference = _exemplar_preference\n",
    "    df['preference'] = _default_preference\n",
    "\n",
    "for common_tag in common_tags:\n",
    "    matching_indices = np.where(df.tags.str.startswith(common_tag))[0]\n",
    "    if len(matching_indices) > 0:\n",
    "        randomly_selected_doc_index = np.random.choice(matching_indices, 1)[0]\n",
    "        logger.info(f'Using doc id {randomly_selected_doc_index} as exemplar based on commonly found tag = {common_tag}')\n",
    "        df.at[randomly_selected_doc_index, 'preference'] = exemplar_preference\n",
    "        df.at[randomly_selected_doc_index, 'is_exemplar'] = True\n",
    "\n",
    "for common_description in common_descriptions:\n",
    "    matching_indices = np.where(df.description == common_description)[0]\n",
    "    if len(matching_indices) > 0:\n",
    "        randomly_selected_doc_index = np.random.choice(matching_indices, 1)[0]\n",
    "        logger.info(f'Using doc id {randomly_selected_doc_index} as exemplar based on commonly found description = {common_description}')\n",
    "        df.at[randomly_selected_doc_index, 'preference'] = exemplar_preference\n",
    "        df.at[randomly_selected_doc_index, 'is_exemplar'] = True\n",
    "\n",
    "logger.info(f'Total no. of exemplars set = {len(df[df.is_exemplar==True])}')\n",
    "logger.info(f'Recreating AffinityPropagation after setting preference={exemplar_preference} for exemplars')\n",
    "\n",
    "af = AffinityPropagation(\n",
    "    verbose=True,\n",
    "    preference=df['preference'].to_numpy(),\n",
    "    damping=PARAMS['AFFINITY_DAMPING'],\n",
    "    max_iter=PARAMS['AFFINITY_MAX_ITER']\n",
    ").fit(X)\n",
    "\n",
    "generate_html(df_complete, wv, af, X, output_dir, 'affinity_clustering_with_preferences.html')\n",
    "# generate_cluster_size_figure(af=af, output_dir=output_dir, filename='affinity_clustering_with_preferences.png')\n",
    "with open(os.path.join(output_dir, 'labels.pik'), 'wb') as f:\n",
    "    pickle.dump(af.labels_, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geniza [~/.conda/envs/geniza/]",
   "language": "python",
   "name": "conda_geniza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
