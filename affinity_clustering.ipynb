{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c75679",
   "metadata": {},
   "source": [
    "## Tweakable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09509b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = dict(\n",
    "    \n",
    "    MAX_DOCS=None,                    # for quick code testing - int or None (all docs)\n",
    "    MIN_LEN=3,                        # words less than this length will be filtered\n",
    "    MAX_LEN=100,                      # words more than this length will be filtered\n",
    "    \n",
    "    DISALLOWED_NERS=[                 # Named-entities to filter out\n",
    "                                      # See https://github.com/explosion/spaCy/blob/b7ba7f78a28ef71fca60415d0165e27a058d1946/spacy/glossary.py#L318\n",
    "        'PERSON',\n",
    "        'GPE',\n",
    "        'ORG'\n",
    "    ],\n",
    "    BIGRAM=False,                     # Form bigrams before creating corpus?\n",
    "    BIGRAM_MIN_PMI=5,                 # Min. PMI in order to create bigrams (determine by manual inspection of generated bigrams.txt)\n",
    "    BIGRAM_MIN_FREQ=20,               # Min. freq of co-occurring tokens before they can be considered a bigram\n",
    "\n",
    "    COMMON_WORDS_MAX_FREQUENCY=10000, # For root words, the max. frequency beyond which they're not useful\n",
    "    COMMON_WORDS_MAX_DOCS=0.5,        # For root words, max docs (absolute or relative) beyond which they're not useful\n",
    "    COMMON_WORDS_MIN_DOCS=5,          # For root words, min docs (absolute or relative) beyond which they're not useful\n",
    "\n",
    "    KEEP_TOKENS=[],                   # Root words to preserve in the vocabulary regardless of their frequency (high or low)\n",
    "\n",
    "\n",
    "    WORD2VEC_VECTOR_SIZE=200,\n",
    "    WORD2VEC_WINDOW=10,\n",
    "    WORD2VEC_EPOCHS=30,\n",
    "\n",
    "    AFFINITY_N_DOCS=None,\n",
    "    AFFINITY_DAMPING=0.7,\n",
    "    AFFINITY_MAX_ITER=400,\n",
    "\n",
    "    # Parameters that determine initial preference values\n",
    "\n",
    "    # How many tags to consider (most common to least common)\n",
    "    AFFINITY_PREFERENCE_N_TAGS=100,\n",
    "\n",
    "    # How many clusters to form based solely on the most common descriptions (before invoking AffinityModel)\n",
    "    N_PRECLUSTERS=30,\n",
    "\n",
    "    # How many documents with common descriptions to use as 'seed' documents for AffinityModel clustering?\n",
    "    AFFINITY_PREFERENCE_N_COMMON_DESCRIPTIONS=20,\n",
    "\n",
    "    # Default preference value for non-exemplar docs - None to auto-determine\n",
    "    AFFINITY_PREFERENCE_DEFAULT=None,\n",
    "\n",
    "    # Default preference for exemplar docs - None to auto-determine\n",
    "    AFFINITY_PREFERENCE_EXEMPLAR=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b95726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vineetb/.conda/envs/geniza/lib/python3.8/site-packages/seaborn/rcmod.py:400: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "/home/vineetb/.conda/envs/geniza/lib/python3.8/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "import textwrap\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "import nltk\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, Word2Vec\n",
    "import spacy\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from ldamallet import LdaMallet\n",
    "\n",
    "\n",
    "logging.basicConfig(level=os.environ.get('LOGLEVEL', 'INFO'))\n",
    "logger = logging.getLogger(__name__)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882ea89",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57b11343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words, disallowed_ners=None, min_len=3, max_len=30):\n",
    "    # python -m spacy download en_core_web_sm\n",
    "    # English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
    "    # Other pipelines at https://spacy.io/models/en\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    texts_out = []\n",
    "\n",
    "    # implement lemmatization and filter out unwanted part of speech tags\n",
    "    for i, sentence in enumerate(texts):\n",
    "        doc = nlp(sentence)\n",
    "        doctext = doc.text\n",
    "        ents = list(doc.ents)\n",
    "\n",
    "        if disallowed_ners is not None:\n",
    "            # Filtering out disallowed NERs should be done prior to splitting the sentence using whitespace.\n",
    "            disallowed_tokens = []\n",
    "            for ent in ents:\n",
    "                if ent.label_ in disallowed_ners:\n",
    "                    disallowed_tokens.append(ent.text.lower())\n",
    "\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "        # simple_preprocess => lowercase; ignore tokens that are too short or too long\n",
    "        tokens = [t for t in simple_preprocess(' '.join(tokens), deacc=False, min_len=min_len, max_len=max_len)\n",
    "                  if t not in stop_words and t not in disallowed_tokens]\n",
    "        texts_out.append(tokens)\n",
    "\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def plot_word2vec_model(model):\n",
    "    words = list(model.wv.key_to_index)\n",
    "    X = model.wv[words]\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(X)\n",
    "    plt.scatter(result[:, 0], result[:, 1])\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "\n",
    "def vectorize(list_of_docs, model):\n",
    "    features = np.zeros((len(list_of_docs), model.vector_size))\n",
    "    for i, tokens in enumerate(list_of_docs):\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        if vectors:\n",
    "            features[i, :] = np.mean(vectors, axis=0)\n",
    "\n",
    "    return features\n",
    "\n",
    "def file_to_set(filepath, split_lines=True):\n",
    "    s = set() \n",
    "    lines = open(filepath, 'r').read().splitlines()\n",
    "    for line in lines:\n",
    "        line = line.strip().lower()\n",
    "        if line and not line.startswith('#'):\n",
    "            if split_lines:\n",
    "                for t in line.split():\n",
    "                    s.add(t)\n",
    "            else:\n",
    "                s.add(line) \n",
    "    return s\n",
    "\n",
    "\n",
    "def generate_html(docs_df, wv, af, X, output_dir, filename):\n",
    "    cluster_centers_indices = af.cluster_centers_indices_\n",
    "    n_clusters_ = len(cluster_centers_indices)\n",
    "    logger.info(f'Estimated number of clusters: {n_clusters_}')\n",
    "\n",
    "    n_preclusters = len(docs_df[docs_df.cluster_id != -1].cluster_id.unique())\n",
    "    # The dataframe representing the docs that af/X see\n",
    "    model_docs_df = docs_df[docs_df.cluster_id == -1]\n",
    "\n",
    "    # Shift label ids identified by the AffinityModel up\n",
    "    labels = n_preclusters + af.labels_\n",
    "\n",
    "    with open(os.path.join(output_dir, filename), 'w') as f:\n",
    "        for i in range(n_preclusters):\n",
    "            _df = docs_df[docs_df.cluster_id == i]\n",
    "            f.write(f'<hr/><b>Cluster {i} ({len(_df)} docs)</b><hr/>')\n",
    "            f.write(f'<i>Pre-cluster based on description:</i> {_df.iloc[0].description}<hr/>')\n",
    "            for _, row in _df.iterrows():\n",
    "                f.write(f'<a target=\"_blank\" href=\"{row.url}\">{row.pgpid}</a><br/>')\n",
    "\n",
    "        for label in np.unique(labels):\n",
    "            docs = np.where(labels == label)[0]\n",
    "            docs_mean_vector = X[docs].mean(axis=0)\n",
    "            terms = ', '.join([term for (term, _) in wv.most_similar(docs_mean_vector)])\n",
    "            f.write(f'<hr/><b>Cluster {label} ({len(docs)} docs)</b><hr/>')\n",
    "            f.write(f'<i>{terms}</i><hr/>')\n",
    "            for doc in docs:\n",
    "                row = model_docs_df.iloc[doc]  # important to use iloc here, not loc\n",
    "                f.write(f'<a target=\"_blank\" href=\"{row.url}\">{row.pgpid}</a><br/>')\n",
    "                f.write('Tags: <i>' + str(row.tags) + '</i>')\n",
    "                if row['is_exemplar']:\n",
    "                    f.write('<p style=\"color:red;\">' + str(row.description) + '</p>')\n",
    "                else:\n",
    "                    f.write('<p>' + str(row.description) + '</p>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaeb7a1",
   "metadata": {},
   "source": [
    "## Unique Run ID\n",
    "\n",
    "The following logic creates a unique Run ID. Parameter values, input and output files are copied to the `results/<run_ID>` folder. You can manually override the Run ID here if you wish to overwrite results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a949e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using Run ID 0002. Results will be stored in the results/0002 folder.\n"
     ]
    }
   ],
   "source": [
    "existing_runs = sorted([d for d in os.listdir('results') if os.path.isdir(f'results/{d}')])\n",
    "if existing_runs:\n",
    "    run_id = int(existing_runs[-1]) + 1\n",
    "else:\n",
    "    run_id = 1\n",
    "# run_id = 42  // Uncomment and specify an explicit Run ID here\n",
    "run_id = f'{run_id:04}'\n",
    "logger.info(f'Using Run ID {run_id}. Results will be stored in the results/{run_id} folder.')\n",
    "\n",
    "output_dir = os.path.join('results', run_id)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger.addHandler(logging.FileHandler(os.path.join(output_dir, 'log.txt')))\n",
    "with open(os.path.join(output_dir, 'params.txt'), 'w') as f:\n",
    "    pprint(PARAMS, f)\n",
    "\n",
    "_stop_words = []\n",
    "os.makedirs(f'{output_dir}/stopwords', exist_ok=True)\n",
    "for filename in glob('stopwords/*.txt'):\n",
    "    _stop_words.extend(list(file_to_set(filename)))\n",
    "    shutil.copy(filename, os.path.join(output_dir, filename))\n",
    "stop_words = _stop_words + nltk.corpus.stopwords.words('english')\n",
    "\n",
    "_bad_tags = []\n",
    "os.makedirs(f'{output_dir}/stoptags', exist_ok=True)\n",
    "for filename in glob('stoptags/*.txt'):\n",
    "    _bad_tags.extend(list(file_to_set(filename)))\n",
    "    shutil.copy(filename, os.path.join(output_dir, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113aaaa",
   "metadata": {},
   "source": [
    "### Read Descriptions\n",
    "\n",
    "Add any calculated columns to the Dataframe here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdbbc45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:No. of records = 5000\n",
      "INFO:__main__:After dropping records with missing description, no. of records = 4967\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('descriptions.csv', dtype={'tags': str})[:PARAMS['MAX_DOCS']]\n",
    "logger.info(f'No. of records = {len(df)}')\n",
    "df = df.dropna(subset=['description'])\n",
    "logger.info(f'After dropping records with missing description, no. of records = {len(df)}')\n",
    "df['tags'] = df['tags'].str.lower()\n",
    "df['tags'].fillna('', inplace=True)\n",
    "\n",
    "# -------------- Add additional columns to Dataframe --------------- #\n",
    "df['cluster_id'] = -1  # will be populated by this script\n",
    "df['preference'] = 0\n",
    "df['is_exemplar'] = False\n",
    "# -------------- Add additional columns to Dataframe --------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6caf2cb",
   "metadata": {},
   "source": [
    "### Find common descriptions\n",
    "\n",
    "Very common descriptions should form their own individual clusters (\"pre-clusters\"). These mostly have descriptions of the form `image missing` / `see Gotein index` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1faa3b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:After filtering out 30 most common descriptions, no. of records = 4865\n"
     ]
    }
   ],
   "source": [
    "descriptions = Counter()\n",
    "for i, row in df.iterrows():\n",
    "    descriptions.update([row.description])\n",
    "\n",
    "common_descriptions_with_counts = descriptions.most_common(PARAMS['N_PRECLUSTERS'] + PARAMS['AFFINITY_PREFERENCE_N_COMMON_DESCRIPTIONS'])\n",
    "common_descriptions = [t[0] for t in common_descriptions_with_counts]\n",
    "for i, desc in enumerate(common_descriptions[:PARAMS['N_PRECLUSTERS']]):\n",
    "    df.loc[df[df.description == desc].index, 'cluster_id'] = i\n",
    "\n",
    "df_complete = df.copy()\n",
    "# Filter these out from further processing\n",
    "df = df[df.cluster_id == -1]\n",
    "logger.info(f'After filtering out {PARAMS[\"N_PRECLUSTERS\"]} most common descriptions, no. of records = {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e9bd94",
   "metadata": {},
   "source": [
    "### Find common tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e10986ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = Counter()\n",
    "tag_dict = {}\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        _tags = row.tags.split(',')\n",
    "    except:\n",
    "        continue\n",
    "    else:\n",
    "        for tag in _tags:\n",
    "            tag = tag.strip().lower().replace(':', '').replace('.', '').replace(';', '').replace('(', '').replace(')', '')\n",
    "            if tag not in _bad_tags:\n",
    "                tags.update([tag])\n",
    "\n",
    "common_tags = [t[0] for t in tags.most_common(PARAMS['AFFINITY_PREFERENCE_N_TAGS'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197cda6",
   "metadata": {},
   "source": [
    "### Create a Word2Vec model\n",
    "\n",
    "After filtering words using POS tagging, lemmatization etc, we create a Word2Vec model with a given vector size, window length etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac0ee709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:After filtering stopwords/short words/lemmatization, no. of records = 4865\n",
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 9791 word types from a corpus of 112458 raw words and 4865 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 2503 unique words (25.564293739148198%% of original 9791, drops 7288)', 'datetime': '2022-06-07T08:46:06.826728', 'gensim': '4.1.2', 'python': '3.8.13 (default, Mar 28 2022, 11:38:47) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 101359 word corpus (90.13053762293478%% of original 112458, drops 11099)', 'datetime': '2022-06-07T08:46:06.829639', 'gensim': '4.1.2', 'python': '3.8.13 (default, Mar 28 2022, 11:38:47) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 9791 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 56 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 84834.93334069665 word corpus (83.7%% of prior 101359)', 'datetime': '2022-06-07T08:46:06.850377', 'gensim': '4.1.2', 'python': '3.8.13 (default, Mar 28 2022, 11:38:47) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 2503 words and 200 dimensions: 5256300 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-06-07T08:46:06.885813', 'gensim': '4.1.2', 'python': '3.8.13 (default, Mar 28 2022, 11:38:47) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 8 workers on 2503 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-06-07T08:46:06.888540', 'gensim': '4.1.2', 'python': '3.8.13 (default, Mar 28 2022, 11:38:47) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 1 - PROGRESS: at 76.55% examples, 61319 words/s, in_qsize 3, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 1 : training on 112458 raw words (84953 effective words) took 1.3s, 67090 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 2 - PROGRESS: at 85.24% examples, 57662 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 2 : training on 112458 raw words (84837 effective words) took 1.2s, 68557 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 3 - PROGRESS: at 85.24% examples, 55536 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 3 : training on 112458 raw words (84832 effective words) took 1.3s, 66437 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 4 - PROGRESS: at 76.55% examples, 60822 words/s, in_qsize 3, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 4 : training on 112458 raw words (84795 effective words) took 1.3s, 66930 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 5 - PROGRESS: at 85.24% examples, 56300 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 5 : training on 112458 raw words (84772 effective words) took 1.3s, 67697 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 6 - PROGRESS: at 85.24% examples, 56344 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 6 : training on 112458 raw words (84864 effective words) took 1.3s, 67630 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 7 - PROGRESS: at 85.24% examples, 58104 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 7 : training on 112458 raw words (84795 effective words) took 1.2s, 69215 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 8 - PROGRESS: at 85.24% examples, 57454 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 8 : training on 112458 raw words (84787 effective words) took 1.2s, 68114 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 9 - PROGRESS: at 85.24% examples, 56294 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 9 : training on 112458 raw words (84737 effective words) took 1.3s, 66551 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 10 - PROGRESS: at 85.24% examples, 56816 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 10 : training on 112458 raw words (84837 effective words) took 1.3s, 67737 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 11 - PROGRESS: at 85.24% examples, 57307 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 11 : training on 112458 raw words (84782 effective words) took 1.3s, 67328 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 12 - PROGRESS: at 85.24% examples, 57653 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 12 : training on 112458 raw words (85036 effective words) took 1.2s, 69215 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 13 - PROGRESS: at 85.24% examples, 57507 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 13 : training on 112458 raw words (84946 effective words) took 1.3s, 67784 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 14 - PROGRESS: at 76.55% examples, 60662 words/s, in_qsize 3, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 14 : training on 112458 raw words (84937 effective words) took 1.3s, 66679 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 15 - PROGRESS: at 85.24% examples, 56599 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 15 : training on 112458 raw words (84714 effective words) took 1.3s, 67198 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 16 - PROGRESS: at 85.24% examples, 56962 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 16 : training on 112458 raw words (84931 effective words) took 1.2s, 68390 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 17 - PROGRESS: at 85.24% examples, 58439 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 17 : training on 112458 raw words (84890 effective words) took 1.2s, 68234 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 18 - PROGRESS: at 76.55% examples, 61647 words/s, in_qsize 3, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 18 : training on 112458 raw words (84746 effective words) took 1.3s, 65320 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 19 - PROGRESS: at 85.24% examples, 56704 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 19 : training on 112458 raw words (84756 effective words) took 1.3s, 67335 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 20 - PROGRESS: at 76.55% examples, 61249 words/s, in_qsize 3, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 20 : training on 112458 raw words (84912 effective words) took 1.3s, 67545 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 21 - PROGRESS: at 85.24% examples, 58848 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 21 : training on 112458 raw words (84720 effective words) took 1.2s, 69585 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 22 - PROGRESS: at 85.24% examples, 57849 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 22 : training on 112458 raw words (84854 effective words) took 1.3s, 67550 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 23 - PROGRESS: at 85.24% examples, 55985 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 23 : training on 112458 raw words (84768 effective words) took 1.3s, 67187 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 24 - PROGRESS: at 85.24% examples, 56302 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 24 : training on 112458 raw words (84812 effective words) took 1.3s, 67101 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 25 - PROGRESS: at 85.24% examples, 58571 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 25 : training on 112458 raw words (84938 effective words) took 1.2s, 69614 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 26 - PROGRESS: at 85.24% examples, 57956 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 26 : training on 112458 raw words (84665 effective words) took 1.2s, 68622 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 27 - PROGRESS: at 76.55% examples, 60910 words/s, in_qsize 3, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 27 : training on 112458 raw words (84819 effective words) took 1.3s, 66279 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 28 - PROGRESS: at 85.24% examples, 57378 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 28 : training on 112458 raw words (84786 effective words) took 1.2s, 68402 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 29 - PROGRESS: at 85.24% examples, 57548 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 29 : training on 112458 raw words (84736 effective words) took 1.2s, 68579 effective words/s\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 7 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 6 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 5 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH 30 - PROGRESS: at 85.24% examples, 58631 words/s, in_qsize 2, out_qsize 1\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "INFO:gensim.models.word2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.word2vec:EPOCH - 30 : training on 112458 raw words (84740 effective words) took 1.2s, 70054 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 3373740 raw words (2544697 effective words) took 37.8s, 67394 effective words/s', 'datetime': '2022-06-07T08:46:44.650166', 'gensim': '4.1.2', 'python': '3.8.13 (default, Mar 28 2022, 11:38:47) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec(vocab=2503, vector_size=200, alpha=0.025)', 'datetime': '2022-06-07T08:46:44.650859', 'gensim': '4.1.2', 'python': '3.8.13 (default, Mar 28 2022, 11:38:47) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'created'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname_or_handle': 'results/0002/wvmodel.bin', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-06-07T08:46:44.654430', 'gensim': '4.1.2', 'python': '3.8.13 (default, Mar 28 2022, 11:38:47) \\n[GCC 7.5.0]', 'platform': 'Linux-4.18.0-348.23.1.el8_5.x86_64-x86_64-with-glibc2.17', 'event': 'saving'}\n",
      "INFO:gensim.utils:not storing attribute cum_table\n",
      "INFO:gensim.utils:saved results/0002/wvmodel.bin\n"
     ]
    }
   ],
   "source": [
    "data_pkl_file = os.path.join(output_dir, 'data.pik')\n",
    "if not os.path.exists(data_pkl_file):\n",
    "    data = list(df.description)\n",
    "    data = process_words(\n",
    "        data,\n",
    "        stop_words=stop_words,\n",
    "        disallowed_ners=PARAMS['DISALLOWED_NERS'],\n",
    "        min_len=PARAMS['MIN_LEN'],\n",
    "        max_len=PARAMS['MAX_LEN']\n",
    "    )\n",
    "    logger.info(f'After filtering stopwords/short words/lemmatization, no. of records = {len(data)}')\n",
    "\n",
    "    with open(data_pkl_file, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(data_pkl_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "if not os.path.exists(os.path.join(output_dir, 'wvmodel.bin')):\n",
    "    model = Word2Vec(\n",
    "        sentences=data,\n",
    "        vector_size=PARAMS['WORD2VEC_VECTOR_SIZE'],\n",
    "        workers=8,\n",
    "        sg=1,\n",
    "        window=PARAMS['WORD2VEC_WINDOW'],\n",
    "        epochs=PARAMS['WORD2VEC_EPOCHS']\n",
    "    )\n",
    "    model.save(os.path.join(output_dir, 'wvmodel.bin'))\n",
    "    with open(os.path.join(output_dir, 'wvmodel_keys.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(model.wv.index_to_key))\n",
    "else:\n",
    "    model = Word2Vec.load(os.path.join(output_dir, 'wvmodel.bin'))\n",
    "\n",
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49054554",
   "metadata": {},
   "source": [
    "### Run AffinityPropagation (first pass)\n",
    "\n",
    "We run AffinityPropagation once to determine the final converged preference values. This will allow us to set the *initial preference* of the documents and the exemplars to suitable values such that exemplars are likely to end up as centers of clusters.\n",
    "\n",
    "For a detailed explanation, see http://genes.toronto.edu/affinitypropagation/faq.html#clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccc9e2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Vectorization of None documents done.\n",
      "INFO:__main__:Using doc id 3444 as exemplar based on commonly found tag = \n",
      "INFO:__main__:Using doc id 1076 as exemplar based on commonly found tag = communal\n",
      "INFO:__main__:Using doc id 2612 as exemplar based on commonly found tag = trade\n",
      "INFO:__main__:Using doc id 2167 as exemplar based on commonly found tag = illness letter 9691517\n",
      "INFO:__main__:Using doc id 1013 as exemplar based on commonly found tag = 11th c\n",
      "INFO:__main__:Using doc id 798 as exemplar based on commonly found tag = account\n",
      "INFO:__main__:Using doc id 4043 as exemplar based on commonly found tag = india\n",
      "INFO:__main__:Using doc id 1512 as exemplar based on commonly found tag = marriage\n",
      "INFO:__main__:Using doc id 3363 as exemplar based on commonly found tag = ketubba\n",
      "INFO:__main__:Using doc id 2104 as exemplar based on commonly found tag = petition\n",
      "INFO:__main__:Using doc id 1401 as exemplar based on commonly found tag = poverty\n",
      "INFO:__main__:Using doc id 133 as exemplar based on commonly found tag = 12th c\n",
      "INFO:__main__:Using doc id 280 as exemplar based on commonly found tag = qodesh\n",
      "INFO:__main__:Using doc id 1933 as exemplar based on commonly found tag = personal\n",
      "INFO:__main__:Using doc id 2018 as exemplar based on commonly found tag = register\n",
      "INFO:__main__:Using doc id 1794 as exemplar based on commonly found tag = waqf\n",
      "INFO:__main__:Using doc id 4248 as exemplar based on commonly found tag = tax\n",
      "INFO:__main__:Using doc id 4317 as exemplar based on commonly found tag = flax\n",
      "INFO:__main__:Using doc id 2180 as exemplar based on commonly found tag = 13th c\n",
      "INFO:__main__:Using doc id 649 as exemplar based on commonly found tag = testimony\n",
      "INFO:__main__:Using doc id 849 as exemplar based on commonly found tag = responsum\n",
      "INFO:__main__:Using doc id 4113 as exemplar based on commonly found tag = ib2\n",
      "INFO:__main__:Using doc id 2439 as exemplar based on commonly found tag = arabic script\n",
      "INFO:__main__:Using doc id 694 as exemplar based on commonly found tag = partnership\n",
      "INFO:__main__:Using doc id 684 as exemplar based on commonly found tag = business\n",
      "INFO:__main__:Using doc id 2397 as exemplar based on commonly found tag = draft\n",
      "INFO:__main__:Using doc id 250 as exemplar based on commonly found tag = slave\n",
      "INFO:__main__:Using doc id 406 as exemplar based on commonly found tag = bill of sale\n",
      "INFO:__main__:Using doc id 638 as exemplar based on commonly found tag = capitation tax\n",
      "INFO:__main__:Using doc id 942 as exemplar based on commonly found tag = divorce\n",
      "INFO:__main__:Using doc id 2098 as exemplar based on commonly found tag = women's letters\n",
      "INFO:__main__:Using doc id 643 as exemplar based on commonly found tag = power of attorney\n",
      "INFO:__main__:Using doc id 1931 as exemplar based on commonly found tag = state\n",
      "INFO:__main__:Using doc id 2407 as exemplar based on commonly found tag = appeal\n",
      "INFO:__main__:Using doc id 626 as exemplar based on commonly found tag = silk\n",
      "INFO:__main__:Using doc id 3368 as exemplar based on commonly found tag = babylonian geonim\n",
      "INFO:__main__:Using doc id 3267 as exemplar based on commonly found tag = inheritance\n",
      "INFO:__main__:Using doc id 1372 as exemplar based on commonly found tag = poem\n",
      "INFO:__main__:Using doc id 2395 as exemplar based on commonly found tag = qaraite\n",
      "INFO:__main__:Using doc id 22 as exemplar based on commonly found tag = al-tahirti\n",
      "INFO:__main__:Using doc id 531 as exemplar based on commonly found tag = will\n",
      "INFO:__main__:Using doc id 3837 as exemplar based on commonly found tag = jewish community\n",
      "INFO:__main__:Using doc id 938 as exemplar based on commonly found tag = epidemic\n",
      "INFO:__main__:Using doc id 2060 as exemplar based on commonly found tag = recommendation\n",
      "INFO:__main__:Using doc id 2714 as exemplar based on commonly found tag = informal note\n",
      "INFO:__main__:Using doc id 2930 as exemplar based on commonly found tag = 14th c\n",
      "INFO:__main__:Using doc id 121 as exemplar based on commonly found tag = arabic address\n",
      "INFO:__main__:Using doc id 407 as exemplar based on commonly found tag = real estate\n",
      "INFO:__main__:Using doc id 936 as exemplar based on commonly found tag = tyre\n",
      "INFO:__main__:Using doc id 2448 as exemplar based on commonly found tag = mosul nasis\n",
      "INFO:__main__:Using doc id 2964 as exemplar based on commonly found tag = legal query\n",
      "INFO:__main__:Using doc id 1087 as exemplar based on commonly found tag = sicily\n",
      "INFO:__main__:Using doc id 1954 as exemplar based on commonly found tag = engagement\n",
      "INFO:__main__:Using doc id 64 as exemplar based on commonly found tag = tiberias\n",
      "INFO:__main__:Using doc id 2017 as exemplar based on commonly found tag = dowry\n",
      "INFO:__main__:Using doc id 4267 as exemplar based on commonly found tag = nr\n",
      "INFO:__main__:Using doc id 2743 as exemplar based on commonly found tag = list\n",
      "INFO:__main__:Using doc id 4386 as exemplar based on commonly found tag = books\n",
      "INFO:__main__:Using doc id 511 as exemplar based on commonly found tag = captives\n",
      "INFO:__main__:Using doc id 4379 as exemplar based on commonly found tag = excommunication\n",
      "INFO:__main__:Using doc id 1145 as exemplar based on commonly found tag = yeshiva\n",
      "INFO:__main__:Using doc id 2438 as exemplar based on commonly found tag = slaves\n",
      "INFO:__main__:Using doc id 3461 as exemplar based on commonly found tag = byzantines\n",
      "INFO:__main__:Using doc id 1856 as exemplar based on commonly found tag = ibn_al-bab\n",
      "INFO:__main__:Using doc id 2624 as exemplar based on commonly found tag = deathbed will\n",
      "INFO:__main__:Using doc id 77 as exemplar based on commonly found tag = jerusalem\n",
      "INFO:__main__:Using doc id 1473 as exemplar based on commonly found tag = alexandria\n",
      "INFO:__main__:Using doc id 1022 as exemplar based on commonly found tag = debt\n",
      "INFO:__main__:Using doc id 1519 as exemplar based on commonly found tag = betrothal\n",
      "INFO:__main__:Using doc id 1407 as exemplar based on commonly found tag = coptic numerals\n",
      "INFO:__main__:Using doc id 985 as exemplar based on commonly found tag = maimonides\n",
      "INFO:__main__:Using doc id 4642 as exemplar based on commonly found tag = family\n",
      "INFO:__main__:Using doc id 2730 as exemplar based on commonly found description = Rescript of the judges of the court appointed by the Nagid Mevorakh to the community of al-Mahalla, advising the community in strongest terms to take back their judge who had left. Dated ca. 1105. (Information from Mediterranean Society, II, 193, 561; V, 49, 519, 542 and from Cohen)\n",
      "INFO:__main__:Using doc id 3185 as exemplar based on commonly found description = Copy of the story of Natan b. Yiaq the Babylonian, from 'Akhbar Baghdad.' In Judaeo-Arabic, in the hand of Natan (ha-nezer) b. Shemuel. This is a mid 12th-century copy of a mid 10th-century story. (Information from Gil, Kingdom, Vol. 2, p. 40.)\n",
      "INFO:__main__:Using doc id 3795 as exemplar based on commonly found description = Part of a booklet, with section beginning, like a title page, 'List of the Poor of Fustat--may God in his mercy make them rich and help them in his grace and kindness' (in K 15.5). Date is preserved here, on fol. 39, as, 11th of Marheshvan, October 30, 1107 (in App. B 21 Goitein wrote Tuesday, Marheshvan 18 [Nov. 5], but dated is correctly in Mediterranean Society, I, p. 56). The date is written in such a way that it could be tyt or tnt, 1107 or 1147, but the reading 1107 is made sure not only by the handwriting of the clerk [=Hillel ben Eli] but also by the names of various people listed, who are known from other Geniza papers. see ibid, I, p. 405 note 89. '490 pounds, amounting to 539 (loaves of bread), from the baker Ma'ali. Ten (pounds) were added, making a total of 500, namely ten loaves of old bread.' Other leaves from this notebook = fols. 5, 15, and 50. Notice the conspicuous presence of the Rum, Jews probably from Byzantium. In several places the names of the baker Ma'ali and that of another baker, Sadaqa, appear, interrupting the list of names of beneficiaries. The handwriting is that of Avraham b. Aharon, who also wrote App. B 23, 24. (Information from Mediterranean Society, II, p. 443, App. B 21, and from Cohen).\n",
      "INFO:__main__:Using doc id 3901 as exemplar based on commonly found description = Testimony regarding bridal consent to marriage. Two manuscripts that deal with the marriage of a bride named Karam. The two manuscripts contain the bride's consent to the marriage, her receiving the initial marriage gift (Muqaddam) and an appoint of a representative on her behalf. The first manuscript, ENA NS I.95a, from the 3rd of Nissan (Monday, the 14th of March) year 1244, is a testimony of witnesses that the bride gave in their presence her consent to the marriage to the aforementioned Groom, and that she received the initial marriage gift (Muqaddam) and accepted the sum of the delayed marriage gift (Me'ukhar). The story takes an unexpected twist in the second manuscript dated two days afterward. While the wife testified in the previous document that she received the initial marriage gift and accepted the sum of the delayed marriage gift, in the second document she appoints her brother to receive the initial marriage gift and to set the sum of the delayed marriage gift. Since the ending of the first document did not survive it is impossible to determine what took place in those two days. The wife might have wanted to testify that she received the muqaddam but things did not turn out the way they should have (the two sides might have fallen into dispute and the agreement was not signed?) Thus, direct involvement of the bride's wife was required. Another option is that the brother's appointment as representative was carried out informally before receiving the initial marriage gift, but only later was this formalized in court. [NB volume number in shelfmark is roman numeral I, not Arabic numeral 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using doc id 4707 as exemplar based on commonly found description = India Book 4 (Hebrew description below; English to come)\n",
      "INFO:__main__:Using doc id 4025 as exemplar based on commonly found description = Awaiting description - see Goitein notes linked below.\n",
      "INFO:__main__:Using doc id 4035 as exemplar based on commonly found description = The widow of Abu al-Barakat, the son of Yosef Lebdi, buys a sixth of two adjacent stores for 53and three/fourthsdinars. The widow was Sitt al-Sada, daughter of Abu Nasr al-Tinnisi. This same share of the two stores was bought at an earlier date by Abu al-Fadil, a physician, from his two nephews. These nephews retained the right to buy.\n",
      "INFO:__main__:Using doc id 4080 as exemplar based on commonly found description = Panegyric in Honor of Madmun II Cairo, after 1186.\n",
      "INFO:__main__:Using doc id 4121 as exemplar based on commonly found description = Awaiting description.\n",
      "INFO:__main__:Using doc id 4128 as exemplar based on commonly found description = Letter from Madmun b. asan to Avraham Ibn Yiju: safe conduct and assistance for brother. Aden, ca. 1145. (8) [I shall take care of] sending it, if someone departs this [year for] (9) Egypt, since no one arrived last year or [so far this year] (10) from Egypt, because of the [death?] (11) and epidemic prevailing there, for two years in a row.\n",
      "INFO:__main__:Using doc id 4146 as exemplar based on commonly found description = Account by Avraham Ibn Yiju of Indian products sold for another merchant, Aden, ca. 1141-44.\n",
      "INFO:__main__:Using doc id 4193 as exemplar based on commonly found description = A booklet containing copies of eight letters by Rav Sherira Gaon and one letter by Rav Hayye Gaon. End of the 10th century. The letter by Rav Hayye Gaon is addressed to the sister of Yosef b. asan b. Bundr, probably a community leader in Northern Iraq. All letters mainly deal with financial matters; the first seven mention places in Yemen. (Information from Gil, Kingdom, Vol. 2, pp. 83-84.)\n",
      "INFO:__main__:Using doc id 4305 as exemplar based on commonly found description = Letter from Mardk b. Ms from Alexandria to Nahray b. Nissim, Fustat. August 10, 1047. Regarding shipments of goods and money. Mentions events related to the Bedouin in Egypt. (Information from Gil, Kingdom, Vol. 3, p. 778.)\n",
      "INFO:__main__:Using doc id 4323 as exemplar based on commonly found description = See also: ENA NS 8.4. Recto: Fragment of a letter from Yosef b. Musa al-Tahirti, from Alexandria, to an unknown person. Around 1060. The addressee is not Nahray, as Nahrays name is mentioned in the letter. The letter contains details about shipments of different goods. The letter is written on the other side of a different letter, that Nissim b. Yishaq al-Tahirti wrote around 10 years earlier. Verso: Letter from Nissim b. Yishaq al-Tahirti from Mahdiyya, probably to Nahray b. Nissim. Around 1050. Nahray buys flax in Egypt and is about to travel to the Maghreb. Nissim writes him details about different goods. He is worried because he did not hear from the ship b. Bader, which his goods are on it. (Information from Gil, Kingdom, Vol. 3, #376) VMR\n",
      "INFO:__main__:Using doc id 4657 as exemplar based on commonly found description = Two fragments (a, b) of a letter from Hillel b. asan, from Alexandria, to Nehoray b. Nisim, Fustat. Around 1070. Regarding a shipment of books, including a book by Nisim b. Yaakov, and a purchase of wheat that were kept in jags. (Information from Gil, Kingdom, Vol. 4, #716) VMR\n",
      "INFO:__main__:Using doc id 4709 as exemplar based on commonly found description = Business letter in the hand of Daniel b. Azarya. On the back are lines in Arabic script and then Judaeo-Arabic in very large spacing. Ed. Gil, Palestine, doc. 347\n",
      "INFO:__main__:Using doc id 4714 as exemplar based on commonly found description = Duplicate, see PGPID 1480\n",
      "INFO:__main__:Using doc id 0 as exemplar based on commonly found description = Fragment of a draft letter in the name of the community of Fustat regarding the imposition of an excommunication, ca. 1035.\n",
      "INFO:__main__:Using doc id 1 as exemplar based on commonly found description = Fragment of a legal document in which the signature of Toviyya b. Daniel appears.\n",
      "INFO:__main__:Using doc id 2 as exemplar based on commonly found description = A writ of qiddush ('betrothal'), Tyre, ca. 1011-1037.\n",
      "INFO:__main__:Total no. of exemplars set = 92\n"
     ]
    }
   ],
   "source": [
    "n_docs = PARAMS['AFFINITY_N_DOCS']\n",
    "X = vectorize(data[:n_docs], model=model)\n",
    "logger.info(f'Vectorization of {n_docs} documents done.')\n",
    "\n",
    "_default_preference = PARAMS['AFFINITY_PREFERENCE_DEFAULT']\n",
    "_exemplar_preference = PARAMS['AFFINITY_PREFERENCE_EXEMPLAR']\n",
    "\n",
    "if _default_preference is None and _exemplar_preference is None:\n",
    "    logger.info('Fitting AffinityPropagation model to documents..')\n",
    "    af = AffinityPropagation(verbose=True, damping=PARAMS['AFFINITY_DAMPING']).fit(X)\n",
    "    generate_html(df_complete, wv, af, X, output_dir, 'affinity_clustering.html')\n",
    "\n",
    "    # If we didn't specify starting preference values, it would have been the median (for all data points):\n",
    "    median_preference = np.median(af.affinity_matrix_)\n",
    "    logger.info(f'Median Preference Value of AF model: {median_preference}')\n",
    "    min_preference = np.min(af.affinity_matrix_)\n",
    "    logger.info(f'Min Preference Value of AF model: {min_preference}')\n",
    "    max_preference = np.max(af.affinity_matrix_)\n",
    "    logger.info(f'Max Preference Value of AF model: {max_preference}')\n",
    "\n",
    "    exemplar_preference = max_preference\n",
    "    df['preference'] = min_preference - (max_preference - min_preference)\n",
    "else:\n",
    "    exemplar_preference = _exemplar_preference\n",
    "    df['preference'] = _default_preference\n",
    "\n",
    "for common_tag in common_tags:\n",
    "    matching_indices = np.where(df.tags.str.startswith(common_tag))[0]\n",
    "    if len(matching_indices) > 0:\n",
    "        randomly_selected_doc_index = np.random.choice(matching_indices, 1)[0]\n",
    "        logger.info(f'Using doc id {randomly_selected_doc_index} as exemplar based on commonly found tag = {common_tag}')\n",
    "        df.at[randomly_selected_doc_index, 'preference'] = exemplar_preference\n",
    "        df.at[randomly_selected_doc_index, 'is_exemplar'] = True\n",
    "\n",
    "for common_description in common_descriptions:\n",
    "    matching_indices = np.where(df.description == common_description)[0]\n",
    "    if len(matching_indices) > 0:\n",
    "        randomly_selected_doc_index = np.random.choice(matching_indices, 1)[0]\n",
    "        logger.info(f'Using doc id {randomly_selected_doc_index} as exemplar based on commonly found description = {common_description}')\n",
    "        df.at[randomly_selected_doc_index, 'preference'] = exemplar_preference\n",
    "        df.at[randomly_selected_doc_index, 'is_exemplar'] = True\n",
    "\n",
    "logger.info(f'Total no. of exemplars set = {len(df[df.is_exemplar==True])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5edfe2",
   "metadata": {},
   "source": [
    "### Re-run AffinityPropagation after setting initial preference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91dabfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Recreating AffinityPropagation after setting preference=0 for exemplars\n",
      "/home/vineetb/.conda/envs/geniza/lib/python3.8/site-packages/sklearn/cluster/_affinity_propagation.py:256: ConvergenceWarning: Affinity propagation did not converge and this model will not have any cluster centers.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Estimated number of clusters: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not converge\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Recreating AffinityPropagation after setting preference={exemplar_preference} for exemplars')\n",
    "\n",
    "af = AffinityPropagation(\n",
    "    verbose=True,\n",
    "    preference=df['preference'].to_numpy(),\n",
    "    damping=PARAMS['AFFINITY_DAMPING'],\n",
    "    max_iter=PARAMS['AFFINITY_MAX_ITER']\n",
    ").fit(X)\n",
    "\n",
    "generate_html(df_complete, wv, af, X, output_dir, 'affinity_clustering_with_preferences.html')\n",
    "with open(os.path.join(output_dir, 'labels.pik'), 'wb') as f:\n",
    "    pickle.dump(af.labels_, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05b0fd",
   "metadata": {},
   "source": [
    "### Analyze clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bcb98b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The resulting clusters are saved in the folder [results/0002](results/0002/affinity_clustering_with_preferences.html)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(f\"The resulting clusters are saved in the folder [results/{run_id}](results/{run_id}/affinity_clustering_with_preferences.html)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geniza [~/.conda/envs/geniza/]",
   "language": "python",
   "name": "conda_geniza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
